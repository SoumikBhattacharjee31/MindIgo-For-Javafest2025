import spacy
import re
from typing import Dict, List, Tuple
from dataclasses import dataclass
from enum import Enum
import logging

# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class IntentType(Enum):
    GREETING = "greeting"
    CRISIS_SELF_HARM = "crisis_self_harm"
    OFF_TOPIC = "off_topic"
    MOOD_SEEKING = "mood_seeking"
    UNKNOWN = "unknown"

class CrisisLevel(Enum):
    NONE = "none"
    MILD_CONCERN = "mild_concern"
    MODERATE_RISK = "moderate_risk"
    HIGH_RISK = "high_risk"
    IMMEDIATE_DANGER = "immediate_danger"

@dataclass
class NLPResult:
    """Structured output for downstream gemini router"""
    
    # Primary intent classification
    primary_intent: IntentType
    confidence: float
    
    # Detailed analysis
    is_greeting: bool
    greeting_confidence: float
    
    crisis_level: CrisisLevel
    crisis_confidence: float
    crisis_indicators: List[str]
    
    is_off_topic: bool
    off_topic_confidence: float
    off_topic_reasons: List[str]
    
    is_mood_seeking: bool
    mood_seeking_confidence: float
    mood_seeking_type: str  # "analysis", "advice", "doctor", "general"
    
    # Text analysis
    sentiment_score: float  # -1 to 1
    emotion_scores: Dict[str, float]
    processed_text: str
    key_phrases: List[str]
    
    # Metadata for routing decisions
    requires_immediate_attention: bool
    complexity_level: str  # "simple", "moderate", "complex"
    recommended_model: str  # "spacy_only", "gemini_light", "gemini_full"

class MentalHealthNLPPipeline:
    """Advanced NLP Pipeline for Mental Health Chat Analysis"""
    
    def __init__(self, model_name: str = "en_core_web_lg"):
        """Initialize the NLP pipeline with spaCy model"""
        try:
            self.nlp = spacy.load(model_name)
            logger.info(f"Loaded spaCy model: {model_name}")
        except OSError:
            logger.error(f"Model {model_name} not found. Install it with: python -m spacy download {model_name}")
            raise
        
        # Initialize pattern libraries
        self._initialize_patterns()
        
    def _initialize_patterns(self):
        """Initialize pattern dictionaries for classification"""
        
        # Greeting patterns
        self.greeting_patterns = {
            'casual': ['hi', 'hello', 'hey', 'howdy', 'sup', 'yo'],
            'formal': ['good morning', 'good afternoon', 'good evening', 'greetings'],
            'questions': ['how are you', 'how do you do', 'what\'s up', 'how\'s it going'],
            'farewells': ['bye', 'goodbye', 'see you', 'take care', 'farewell', 'later']
        }
        
        # Crisis/Self-harm indicators (organized by severity)
        self.crisis_patterns = {
            'immediate_danger': [
                'kill myself', 'end my life', 'suicide', 'want to die', 'going to die',
                'overdose', 'jump off', 'hanging myself', 'cutting deep', 'final solution'
            ],
            'high_risk': [
                'better off dead', 'life isn\'t worth', 'nothing matters anymore',
                'permanent solution', 'end the pain', 'can\'t go on', 'give up on life',
                'hurt myself badly', 'serious self harm'
            ],
            'moderate_risk': [
                'hurt myself', 'self harm', 'cut myself', 'pain helps', 'deserve pain',
                'worthless', 'burden to everyone', 'hopeless', 'no way out',
                'tired of living', 'everyone hates me'
            ],
            'mild_concern': [
                'feeling down', 'really sad', 'depressed', 'anxious', 'overwhelmed',
                'stressed out', 'can\'t cope', 'struggling', 'having a hard time'
            ]
        }
        
        # Off-topic patterns (not related to mental health/personal wellbeing)
        self.off_topic_patterns = {
            'technology': ['computer', 'software', 'programming', 'coding', 'app development'],
            'weather': ['weather', 'rain', 'sunny', 'temperature', 'forecast'],
            'sports': ['football', 'basketball', 'soccer', 'game score', 'match'],
            'politics': ['election', 'government', 'president', 'politics', 'vote'],
            'entertainment': ['movie', 'tv show', 'celebrity', 'music recommendation'],
            'shopping': ['buy', 'purchase', 'shopping', 'price', 'product review'],
            'general_info': ['what is', 'how to', 'define', 'explain', 'tutorial']
        }
        
        # Mood seeking patterns
        self.mood_seeking_patterns = {
            'analysis': [
                'analyze my mood', 'how am i feeling', 'mood analysis', 'emotional state',
                'mental state', 'track my mood', 'mood patterns'
            ],
            'advice': [
                'what should i do', 'need advice', 'help me', 'suggestions', 'recommend',
                'how to feel better', 'cope with', 'deal with', 'manage my'
            ],
            'doctor': [
                'find a doctor', 'therapist', 'psychiatrist', 'counselor', 'mental health professional',
                'therapy', 'treatment', 'medication', 'professional help'
            ],
            'general': [
                'feeling', 'emotion', 'mood', 'mental health', 'wellbeing', 'support'
            ]
        }

    def preprocess_text(self, text: str) -> str:
        """
        Advanced text preprocessing for chat text
        Removes unnecessary characters while preserving emotional context
        """
        if not text or not isinstance(text, str):
            return ""
        
        # Store original for comparison
        original_text = text
        
        # 1. Handle excessive punctuation (preserve emotional emphasis)
        # Replace multiple exclamation marks with just two
        text = re.sub(r'!{3,}', '!!', text)
        # Replace multiple question marks with just two
        text = re.sub(r'\?{3,}', '??', text)
        # Handle excessive dots (but preserve ellipsis)
        text = re.sub(r'\.{4,}', '...', text)
        
        # 2. Clean up common chat artifacts
        # Remove excessive spaces
        text = re.sub(r'\s+', ' ', text)
        # Remove leading/trailing whitespace
        text = text.strip()
        
        # 3. Handle common typos and chat speak (preserve emotional context)
        # Common emotional expressions
        typo_corrections = {
            r'\bu\b': 'you',
            r'\bur\b': 'your',
            r'\br\b': 'are',
            r'\btho\b': 'though',
            r'\bw/\b': 'with',
            r'\bw/o\b': 'without',
            r'\bbcuz\b': 'because',
            r'\bcuz\b': 'because',
            r'\bidk\b': 'i don\'t know',
            r'\bomg\b': 'oh my god',
            r'\bbtw\b': 'by the way',
        }
        
        for pattern, replacement in typo_corrections.items():
            text = re.sub(pattern, replacement, text, flags=re.IGNORECASE)
        
        # 4. Remove or clean problematic characters
        # Remove excessive special characters but keep emotionally relevant ones
        # Keep: . , ! ? ' " - ( ) : ; 
        # Remove excessive symbols
        text = re.sub(r'[#$%&*+/<=>@\[\\\]^_`{|}~]{2,}', '', text)
        
        # 5. Handle numbers (convert some to words for better processing)
        # Convert single digits to words for emotional context
        number_words = {
            '0': 'zero', '1': 'one', '2': 'two', '3': 'three', '4': 'four',
            '5': 'five', '6': 'six', '7': 'seven', '8': 'eight', '9': 'nine'
        }
        # Only convert standalone single digits
        for num, word in number_words.items():
            text = re.sub(rf'\b{num}\b', word, text)
        
        # 6. Clean up excessive capitalization (preserve some emphasis)
        # Convert FULL CAPS words to Title Case (preserving emotional emphasis)
        words = text.split()
        processed_words = []
        for word in words:
            if len(word) > 2 and word.isupper() and word.isalpha():
                # Keep it emphasized but readable
                processed_words.append(word.title())
            else:
                processed_words.append(word)
        text = ' '.join(processed_words)
        
        # 7. Final cleanup
        # Remove any remaining excessive whitespace
        text = re.sub(r'\s+', ' ', text).strip()
        
        # Log significant changes for debugging
        if len(original_text) - len(text) > len(original_text) * 0.3:
            logger.info(f"Significant preprocessing changes: '{original_text[:50]}...' -> '{text[:50]}...'")
        
        return text

    def extract_key_phrases(self, doc) -> List[str]:
        """Extract meaningful phrases using spaCy's linguistic features"""
        key_phrases = []
        
        # Extract noun phrases
        for chunk in doc.noun_chunks:
            if len(chunk.text.split()) >= 2:  # Multi-word phrases
                key_phrases.append(chunk.text.lower().strip())
        
        # Extract verb phrases with emotional content
        emotional_verbs = ['feel', 'think', 'believe', 'want', 'need', 'hurt', 'love', 'hate', 'fear']
        for token in doc:
            if token.lemma_ in emotional_verbs and token.head != token:
                phrase = f"{token.text} {token.head.text}"
                key_phrases.append(phrase.lower().strip())
        
        # Remove duplicates and short phrases
        key_phrases = list(set([phrase for phrase in key_phrases if len(phrase) > 3]))
        
        return key_phrases[:10]  # Return top 10

    def analyze_greeting(self, text: str, doc) -> Tuple[bool, float]:
        """Analyze if text contains greeting patterns"""
        text_lower = text.lower()
        
        # Check for exact matches in greeting patterns
        confidence = 0.0
        
        for category, patterns in self.greeting_patterns.items():
            for pattern in patterns:
                if pattern in text_lower:
                    if category == 'casual':
                        confidence = max(confidence, 0.9)
                    elif category == 'formal':
                        confidence = max(confidence, 0.95)
                    elif category == 'questions':
                        confidence = max(confidence, 0.85)
                    elif category == 'farewells':
                        confidence = max(confidence, 0.8)
        
        # Additional checks for short, greeting-like messages
        if len(text.split()) <= 3 and confidence > 0.7:
            confidence = min(confidence + 0.1, 1.0)
        
        # Check sentence structure for greeting patterns
        if doc and len(list(doc.sents)) == 1:
            sent = list(doc.sents)[0]
            if len(sent) <= 5 and any(token.pos_ in ['INTJ', 'NOUN'] for token in sent):
                confidence = max(confidence, 0.6)
        
        is_greeting = confidence >= 0.7
        return is_greeting, confidence

    def analyze_crisis_level(self, text: str, doc) -> Tuple[CrisisLevel, float, List[str]]:
        """Analyze crisis/self-harm indicators with severity levels"""
        text_lower = text.lower()
        indicators = []
        max_confidence = 0.0
        crisis_level = CrisisLevel.NONE
        
        # Check each crisis level
        for level, patterns in self.crisis_patterns.items():
            level_confidence = 0.0
            level_indicators = []
            
            for pattern in patterns:
                if pattern in text_lower:
                    level_indicators.append(pattern)
                    # Calculate confidence based on pattern specificity
                    if level == 'immediate_danger':
                        level_confidence = max(level_confidence, 0.95)
                    elif level == 'high_risk':
                        level_confidence = max(level_confidence, 0.85)
                    elif level == 'moderate_risk':
                        level_confidence = max(level_confidence, 0.75)
                    elif level == 'mild_concern':
                        level_confidence = max(level_confidence, 0.6)
            
            # Update if this is the highest risk level found
            if level_indicators and level_confidence > max_confidence:
                max_confidence = level_confidence
                indicators = level_indicators
                if level == 'immediate_danger':
                    crisis_level = CrisisLevel.IMMEDIATE_DANGER
                elif level == 'high_risk':
                    crisis_level = CrisisLevel.HIGH_RISK
                elif level == 'moderate_risk':
                    crisis_level = CrisisLevel.MODERATE_RISK
                elif level == 'mild_concern':
                    crisis_level = CrisisLevel.MILD_CONCERN
        
        # Additional linguistic analysis for subtle indicators
        if doc:
            # Check for negative sentiment with personal pronouns
            personal_pronouns = [token for token in doc if token.pos_ == 'PRON' and token.text.lower() in ['i', 'me', 'my', 'myself']]
            negative_words = [token for token in doc if token.sentiment < -0.3]
            
            if personal_pronouns and negative_words and crisis_level == CrisisLevel.NONE:
                crisis_level = CrisisLevel.MILD_CONCERN
                max_confidence = max(max_confidence, 0.4)
                indicators.append("negative self-reference pattern")
        
        return crisis_level, max_confidence, indicators

    def analyze_off_topic(self, text: str, doc) -> Tuple[bool, float, List[str]]:
        """Analyze if text is off-topic for mental health context"""
        text_lower = text.lower()
        off_topic_score = 0.0
        reasons = []
        
        # Check against off-topic patterns
        for category, patterns in self.off_topic_patterns.items():
            category_matches = 0
            for pattern in patterns:
                if pattern in text_lower:
                    category_matches += 1
            
            if category_matches > 0:
                category_confidence = min(category_matches * 0.3, 0.9)
                off_topic_score = max(off_topic_score, category_confidence)
                reasons.append(f"{category}_related")
        
        # Additional checks using spaCy NER
        if doc:
            # Check for entities that are typically off-topic
            off_topic_entities = ['ORG', 'GPE', 'PRODUCT', 'EVENT', 'LAW']
            for ent in doc.ents:
                if ent.label_ in off_topic_entities:
                    off_topic_score = max(off_topic_score, 0.6)
                    reasons.append(f"entity_{ent.label_.lower()}")
        
        # Check if it's asking for general information (not personal)
        info_seeking_patterns = ['what is', 'how to', 'define', 'explain']
        for pattern in info_seeking_patterns:
            if pattern in text_lower and 'i' not in text_lower[:10]:  # Not personal
                off_topic_score = max(off_topic_score, 0.7)
                reasons.append("general_information_seeking")
        
        is_off_topic = off_topic_score >= 0.6
        return is_off_topic, off_topic_score, reasons

    def analyze_mood_seeking(self, text: str, doc) -> Tuple[bool, float, str]:
        """Analyze if user is seeking mood analysis, advice, or mental health information"""
        text_lower = text.lower()
        
        max_confidence = 0.0
        seeking_type = "general"
        
        # Check each mood-seeking category
        for category, patterns in self.mood_seeking_patterns.items():
            category_confidence = 0.0
            
            for pattern in patterns:
                if pattern in text_lower:
                    category_confidence = max(category_confidence, 0.8)
            
            # Update if this is the strongest signal
            if category_confidence > max_confidence:
                max_confidence = category_confidence
                seeking_type = category
        
        # Additional linguistic analysis
        if doc:
            # Look for question patterns about self
            question_words = ['what', 'how', 'why', 'when', 'should']
            has_question_word = any(token.text.lower() in question_words for token in doc)
            has_self_reference = any(token.text.lower() in ['i', 'me', 'my'] for token in doc)
            
            if has_question_word and has_self_reference:
                max_confidence = max(max_confidence, 0.7)
                if seeking_type == "general":
                    seeking_type = "advice"
        
        is_mood_seeking = max_confidence >= 0.6
        return is_mood_seeking, max_confidence, seeking_type

    def calculate_sentiment(self, doc) -> float:
        """Calculate overall sentiment score using spaCy"""
        if not doc:
            return 0.0
        
        # Simple sentiment calculation based on token sentiment
        sentiments = [token.sentiment for token in doc if hasattr(token, 'sentiment')]
        
        if sentiments:
            return sum(sentiments) / len(sentiments)
        
        # Fallback: rule-based sentiment
        positive_words = ['good', 'great', 'happy', 'better', 'hope', 'love', 'joy']
        negative_words = ['bad', 'terrible', 'sad', 'worse', 'hate', 'pain', 'hurt']
        
        text_lower = doc.text.lower()
        pos_count = sum(1 for word in positive_words if word in text_lower)
        neg_count = sum(1 for word in negative_words if word in text_lower)
        
        total_words = len([token for token in doc if token.is_alpha])
        if total_words == 0:
            return 0.0
        
        return (pos_count - neg_count) / total_words

    def extract_emotion_scores(self, text: str, doc) -> Dict[str, float]:
        """Extract emotion scores using pattern matching"""
        emotions = {
            'joy': ['happy', 'joy', 'excited', 'cheerful', 'glad', 'pleased'],
            'sadness': ['sad', 'depressed', 'down', 'blue', 'unhappy', 'melancholy'],
            'anger': ['angry', 'mad', 'furious', 'irritated', 'annoyed', 'rage'],
            'fear': ['scared', 'afraid', 'terrified', 'anxious', 'worried', 'nervous'],
            'surprise': ['surprised', 'shocked', 'amazed', 'astonished'],
            'disgust': ['disgusted', 'revolted', 'repulsed', 'sick'],
        }
        
        text_lower = text.lower()
        emotion_scores = {}
        
        for emotion, words in emotions.items():
            score = sum(1 for word in words if word in text_lower)
            # Normalize by text length
            emotion_scores[emotion] = min(score / max(len(text.split()), 1), 1.0)
        
        return emotion_scores

    def determine_complexity_and_routing(self, result: NLPResult) -> Tuple[str, str]:
        """Determine complexity level and recommended model based on analysis"""
        
        # Immediate attention cases
        if result.crisis_level in [CrisisLevel.HIGH_RISK, CrisisLevel.IMMEDIATE_DANGER]:
            return "complex", "gemini_full"
        
        # High confidence simple cases that spaCy can handle
        if result.is_greeting and result.greeting_confidence >= 0.8:
            return "simple", "spacy_only"
        
        if result.is_off_topic and result.off_topic_confidence >= 0.8:
            return "simple", "spacy_only"
        
        # Moderate complexity cases
        if result.is_mood_seeking or result.crisis_level == CrisisLevel.MODERATE_RISK:
            return "moderate", "gemini_light"
        
        # Low confidence or ambiguous cases need full analysis
        if result.confidence < 0.7:
            return "complex", "gemini_full"
        
        # Default to moderate
        return "moderate", "gemini_light"

    def analyze(self, text: str) -> NLPResult:
        """
        Main analysis function that processes text and returns structured results
        """
        if not text or not text.strip():
            return self._create_empty_result()
        
        # Preprocess the text
        processed_text = self.preprocess_text(text)
        
        # Process with spaCy
        doc = self.nlp(processed_text)
        
        # Extract all features
        is_greeting, greeting_conf = self.analyze_greeting(processed_text, doc)
        crisis_level, crisis_conf, crisis_indicators = self.analyze_crisis_level(processed_text, doc)
        is_off_topic, off_topic_conf, off_topic_reasons = self.analyze_off_topic(processed_text, doc)
        is_mood_seeking, mood_seeking_conf, mood_seeking_type = self.analyze_mood_seeking(processed_text, doc)
        
        # Calculate primary intent and confidence
        intent_scores = {
            IntentType.GREETING: greeting_conf if is_greeting else 0.0,
            IntentType.CRISIS_SELF_HARM: crisis_conf if crisis_level != CrisisLevel.NONE else 0.0,
            IntentType.OFF_TOPIC: off_topic_conf if is_off_topic else 0.0,
            IntentType.MOOD_SEEKING: mood_seeking_conf if is_mood_seeking else 0.0,
        }
        
        primary_intent = max(intent_scores.items(), key=lambda x: x[1])
        
        # If no clear intent, mark as unknown
        if primary_intent[1] < 0.5:
            primary_intent = (IntentType.UNKNOWN, 0.3)
        
        # Create result object
        result = NLPResult(
            primary_intent=primary_intent[0],
            confidence=primary_intent[1],
            
            is_greeting=is_greeting,
            greeting_confidence=greeting_conf,
            
            crisis_level=crisis_level,
            crisis_confidence=crisis_conf,
            crisis_indicators=crisis_indicators,
            
            is_off_topic=is_off_topic,
            off_topic_confidence=off_topic_conf,
            off_topic_reasons=off_topic_reasons,
            
            is_mood_seeking=is_mood_seeking,
            mood_seeking_confidence=mood_seeking_conf,
            mood_seeking_type=mood_seeking_type,
            
            sentiment_score=self.calculate_sentiment(doc),
            emotion_scores=self.extract_emotion_scores(processed_text, doc),
            processed_text=processed_text,
            key_phrases=self.extract_key_phrases(doc),
            
            requires_immediate_attention=crisis_level in [CrisisLevel.HIGH_RISK, CrisisLevel.IMMEDIATE_DANGER],
            complexity_level="", # Will be set below
            recommended_model="" # Will be set below
        )
        
        # Determine complexity and routing
        complexity, model = self.determine_complexity_and_routing(result)
        result.complexity_level = complexity
        result.recommended_model = model
        
        return result

    def _create_empty_result(self) -> NLPResult:
        """Create empty result for invalid input"""
        return NLPResult(
            primary_intent=IntentType.UNKNOWN,
            confidence=0.0,
            is_greeting=False,
            greeting_confidence=0.0,
            crisis_level=CrisisLevel.NONE,
            crisis_confidence=0.0,
            crisis_indicators=[],
            is_off_topic=False,
            off_topic_confidence=0.0,
            off_topic_reasons=[],
            is_mood_seeking=False,
            mood_seeking_confidence=0.0,
            mood_seeking_type="general",
            sentiment_score=0.0,
            emotion_scores={},
            processed_text="",
            key_phrases=[],
            requires_immediate_attention=False,
            complexity_level="simple",
            recommended_model="spacy_only"
        )

    def get_routing_decision(self, result: NLPResult) -> Dict[str, any]:
        """
        Get structured routing decision for downstream gemini router
        """
        return {
            "use_spacy_only": result.recommended_model == "spacy_only" and result.confidence >= 0.8,
            "use_gemini_light": result.recommended_model == "gemini_light",
            "use_gemini_full": result.recommended_model == "gemini_full",
            "requires_immediate_attention": result.requires_immediate_attention,
            "confidence": result.confidence,
            "primary_intent": result.primary_intent.value,
            "complexity": result.complexity_level,
            "crisis_level": result.crisis_level.value if result.crisis_level != CrisisLevel.NONE else None,
            "reasoning": self._get_routing_reasoning(result)
        }

    def _get_routing_reasoning(self, result: NLPResult) -> str:
        """Generate reasoning for routing decision"""
        if result.requires_immediate_attention:
            return f"Crisis level {result.crisis_level.value} detected - requires immediate full analysis"
        
        if result.recommended_model == "spacy_only":
            return f"High confidence {result.primary_intent.value} (conf: {result.confidence:.2f}) - spaCy sufficient"
        
        if result.confidence < 0.7:
            return f"Low confidence ({result.confidence:.2f}) - needs gemini analysis"
        
        return f"Moderate complexity {result.primary_intent.value} - gemini light recommended"


# Example usage and testing
if __name__ == "__main__":
    # Initialize pipeline
    pipeline = MentalHealthNLPPipeline()
    
    # Test cases
    test_cases = [
        "Hi there! How are you doing?",
        "I'm feeling really depressed and want to hurt myself",
        "What's the weather like today?",
        "Can you analyze my mood? I've been feeling down lately",
        "I can't take this anymore... I want to end it all",
        "hey whats up!!!!! hope ur doing good :)",
        "I need help finding a therapist in my area",
        "How do I code a Python function?",
    ]
    
    print("=" * 80)
    print("MENTAL HEALTH NLP PIPELINE TEST RESULTS")
    print("=" * 80)
    
    for i, text in enumerate(test_cases, 1):
        print(f"\n--- Test Case {i} ---")
        print(f"Input: '{text}'")
        
        result = pipeline.analyze(text)
        routing = pipeline.get_routing_decision(result)
        
        print(f"Primary Intent: {result.primary_intent.value} (confidence: {result.confidence:.2f})")
        print(f"Crisis Level: {result.crisis_level.value}")
        print(f"Routing: {routing['reasoning']}")
        print(f"Recommended Model: {result.recommended_model}")
        
        if result.crisis_indicators:
            print(f"Crisis Indicators: {result.crisis_indicators}")
        if result.key_phrases:
            print(f"Key Phrases: {result.key_phrases[:3]}")
